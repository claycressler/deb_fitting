\documentclass[12pt,reqno,final,pdftex]{amsart}
%% DO NOT DELETE OR CHANGE THE FOLLOWING TWO LINES!
%% $Revision$
%% $Date$
\usepackage[round,sort,elide]{natbib}
\usepackage{graphicx}
\usepackage{times}
\usepackage{rotating}
\usepackage{subfig}
\usepackage{color}
\newcommand{\aak}[1]{\textcolor{cyan}{#1}}
\newcommand{\mab}[1]{\textcolor{red}{#1}}
\newcommand{\cec}[1]{\textcolor{blue}{#1}}

\setlength{\textwidth}{6.25in}
\setlength{\textheight}{8.75in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{-.35in}
\setlength{\parskip}{.1in}
\setlength{\parindent}{0.3in}

%% cleveref must be last loaded package
\usepackage[sort&compress]{cleveref}
\newcommand{\crefrangeconjunction}{--}
\crefname{figure}{Fig.}{Figs.}
\Crefname{figure}{Fig.}{Figs.}
\crefname{table}{Table}{Tables}
\Crefname{table}{Tab.}{Tables}
\crefname{equation}{Eq.}{Eqs.}
\Crefname{equation}{Eq.}{Eqs.}
\crefname{appendix}{Appendix}{Appendices}
\Crefname{appendix}{Appendix}{Appendices}
\creflabelformat{equation}{#2#1#3}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{corol}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{hyp}[thm]{Hypothesis}
\newtheorem{example}[thm]{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{algorithm}[thm]{Algorithm}
\newtheorem{remark}{Remark}
\renewcommand\thethm{\arabic{thm}}
\renewcommand{\theremark}{}

\numberwithin{equation}{part}
\renewcommand\theequation{\arabic{equation}}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thefigure{\arabic{figure}}
\renewcommand\thetable{\arabic{table}}
\renewcommand\thefootnote{\arabic{footnote}}

\newcommand\scinot[2]{$#1 \times 10^{#2}$}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textsf{#1}}
\newcommand{\dlta}[1]{{\Delta}{#1}}
\newcommand{\Prob}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\Expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[#1\right]}
\newcommand{\dd}[1]{\mathrm{d}{#1}}
\newcommand{\citetpos}[1]{\citeauthor{#1}'s \citeyearpar{#1}}

\begin{document}

<<setup,include=FALSE,cache=F>>=
require(knitr)
opts_chunk$set(
               progress=T,prompt=F,tidy=F,highlight=T,
               warning=F,message=F,error=F,
               results='hide',echo=F,cache=T,
               size='scriptsize',
               fig.path='figure/',fig.lp="fig:",
               fig.align='left',
               fig.show='asis',
               fig.height=4,fig.width=6.83,
               out.width="\\linewidth",
               dpi=150,
               dev=c('png','tiff'),
               dev.args=list(
                 png=list(bg='transparent'),
                 tiff=list(compression='lzw')
                 )
               )

scinot <- function (x, digits = 2, type = c("expression","latex")) {
  type <- match.arg(type)
  x <- signif(x,digits=digits)
  ch <- floor(log10(abs(x)))
  mn <- x/10^ch
  switch(type,
         expression={
           bquote(.(mn)%*%10^.(ch))
         },
         latex={
           paste0("\\scinot{",mn,"}{",ch,"}")
         }
         )
}

require(xtable)

options(scipen=-1)

options(
        xtable.caption.placement="top",
        xtable.include.rownames=FALSE
        )

@

\section*{Fitting the growth and reproduction trajectories of uninfected animals}

Here we fit the DEB model used in the simulation/recovery exercise to data on the growth and reproduction of uninfected (control) \emph{Daphnia}.
To recap, the model we are fitting is the following:
\begin{align}
\frac{dF}{dt} &= I_{max} \frac{F}{F_h+F} L_{obs}^g, \\
\frac{dE}{dt} &= \rho \epsilon V I_{max} \frac{F}{F_h+F} L_{obs}^g - P_C, \\
\frac{dW}{dt} &= \kappa P_C - k_m W, \\
\frac{dM}{dt} &=
\begin{cases}
(1-\kappa) P_C - k_m M \mbox{ if $W < W_{mat}$}, \\
0 \mbox{ if $W >= W_{mat}$},
\end{cases} \\
\frac{dR}{dt} &= \begin{cases} 0 \mbox{ if $W < W_{mat}$}, \\ \frac{(1-\kappa) P_C - k_m M}{E_R} \mbox{ if $W >= W_{mat}$}, \end{cases} \\
P_C &= \frac{E (\nu/L + k_m)}{1 + \kappa E/W}, \\
L &= W^{1/3}, \\
L_{obs} &= \left(\frac{W}{\xi}\right)^{1/q}, \\
F(0) &= F_0, \\
E(0) &= \frac{\rho E_R}{\nu}, \\
W(0) &= E_R, \\
M(0) &= 0, \\
R(0) &= 0.
\end{align}

The other issue that we need to deal with here is the measurement model for growth and reproduction.
It seems reasonable to assume normally distributed measurement error for \emph{Daphnia} size; in this case, the mean of the measurement model is given by the model prediction, and the variance is estimated as a parameter of the fitting.
The measurement model for reproduction, however, is more complicated, and there are several possible models that can be justified.
Recall that the data is cumulative reproduction by an individual.
One possibility is to assume normally distributed measurement error (where the mean is given by the model-predicted cumulative reproduction at a given age), so the fitting algorithm estimates a constant standard deviation parameter.
It is plausible that measurement error for each individual clutch is normally distributed, but the constant standard deviation assumption is problematic for cumulative reproduction because of the range for cumulative reproduction data is quite large.
A second possibility is to assume that measurement error is normally distributed, but rather than assuming a constant standard deviation, instead assume that the coefficient of variation in measurement error is constant.
In this case, the algorithm estimates the coefficient of variation and the measurement model assumes that the mean is given by the model prediction and the standard deviation is the mean times the coefficient of variation.
The third possibility is to assume that cumulative reproduction has a negative binomial error distribution.
In this case, the mean is given by the model prediction and the variance is equal to $\mu + \mu^2/k$, where $k$ is the shape parameter, which is estimated.
Since I don't know which to use, I am going to fit the data with these different measurement models, and let the data tell me which measurement model is best supported.

This reveals that the best-supported measurement model, by a \emph{huge} margin, is the normal distribution model with constant coefficient of variation.
The log-likelihood for the best-fitting parameter set for the model with normally distributed measurement error and constant standard deviation is -209.8; the log-likelihood for the best-fitting parameter set for the model with normally distributed measurement error and constant coefficient of variation is 350.9; the log-likelihood for the best-fitting parameter set for the model with negative binomially distributed measurement error is -19.3.


<<echo=FALSE>>=
x <- readRDS(file="Trajectory_matching_estimates_uninfected_animals_12-6.RDS")
y <- readRDS(file="Trajectory_matching_estimates_uninfected_animals_estCV_5-9.RDS")
z <- readRDS(file="Trajectory_matching_estimates_uninfected_animals_negbinom_5-9.RDS")

library(ggplot2)
library(plyr)
library(dplyr)
library(GGally)

x %>% subset(., lik < min(lik) + 2 & conv==0) %>% mutate(., lik=-lik) -> x
y %>% subset(., lik < min(lik) + 2 & conv==0) %>% mutate(., lik=-lik) -> y
z %>% subset(., lik < min(lik) + 2 & conv==0) %>% mutate(., lik=-lik) -> z

@

However, if you actually look at the model predictions against the data, you can see that the negative binomial model does a much better job of actually fitting the data than does the model with constant coefficient of variation (Fig. \ref{fig:model-data-comp}).
The reason the model with constant coefficient of variation has such a high likelihood compared to the other two is because of the zeros.
The log-likelihood of the zeros is 8 to 10 units higher than any other point at the best-fitting parameter set.
In fact, the estimated CV is very near the CV that would maximize the likelihood when observed and predicted reproduction are both zero, clearly indicating that these values are exerting undue influence on the fitting algorithm.

<<echo=FALSE, fig.height=4, fig.width=6, units='in', fig.cap="Comparing model predictions to observed data. The first row shows the deterministic prediction of growth and reproduction (gray lines) for the model where the error in the observation of reproduction is normally distributed with a constant error through time. The black error bars shows the 95\\% confidence interval in the measurement model at each age where data was observed. The second row shows the same information for the model with normally distributed observation error and a constant coefficient of variation. The third row shows the model with negative binomially distributed error. The red circles are observed data, which largely falls within the confidence interval, except for some of the reproduction data.", label="model-data-comp">>=
## Load Cat's data
library(deSolve)
xx <- read.csv("Cat_data/uninfected_growth_reproduction.csv")
data <- xx[1:103,1:3]
data$eggs[is.na(data$eggs)] <- 0 ## set reproduction = 0 for all individuals that have not yet matured
## change the name of 'times' to 'age'
colnames(data)[1] <- 'age'

source("Growth_reproduction_trajectory_matching_real_data_2.R")
fixpars <- c(Imax=22500, g=1.45, v=10, F0=1e6/30)
## Parameter estimates for the 3 different error models
estparsx <- unlist(x[1,1:8])
estparsy <- unlist(y[1,1:8])
estparsz <- unlist(z[1,1:8])
fixpars["Imax"] <- calc_Imax(unname(estparsx["Fh"]))
fixpars["g"] <- calc_g(unname(estparsx["Fh"]))
fixparsx <- fixpars
fixpars["Imax"] <- calc_Imax(unname(estparsy["Fh"]))
fixpars["g"] <- calc_g(unname(estparsy["Fh"]))
fixparsy <- fixpars
fixpars["Imax"] <- calc_Imax(unname(estparsz["Fh"]))
fixpars["g"] <- calc_g(unname(estparsz["Fh"]))
fixparsz <- fixpars

parorder <- c("Imax","Fh","g","rho","K","km","v","ER","F0","Lobs","Robs","Wmat")
parsx <- c(estparsx, fixparsx)
parsy <- c(estparsy, fixparsy)
parsz <- c(estparsz, fixparsz)
parsx[match(parorder, names(parsx))] -> parsx
parsy[match(parorder, names(parsy))] -> parsy
parsz[match(parorder, names(parsz))] -> parsz

## Set the initial conditions
yx <- c(F=unname(parsx["F0"]),
        E=0,
        W=unname(parsx["ER"]/(1+parsx["rho"]/parsx["v"])),
        M=0,
        R=0)
yx["E"] <- unname(yx["W"]*parsx["rho"]/parsx["v"])
yy <- c(F=unname(parsy["F0"]),
        E=0,
        W=unname(parsy["ER"]/(1+parsy["rho"]/parsy["v"])),
        M=0,
        R=0)
yy["E"] <- unname(yy["W"]*parsy["rho"]/parsy["v"])
yz <- c(F=unname(parsz["F0"]),
        E=0,
        W=unname(parsz["ER"]/(1+parsz["rho"]/parsz["v"])),
        M=0,
        R=0)
yz["E"] <- unname(yz["W"]*parsz["rho"]/parsz["v"])


## Feeding
eventdat <- data.frame(var="F",
                       time=1:35,
                       value=unname(pars["F0"]),
                       method=rep(c(rep("add",4),"rep"),7))
## simulate the system
ode(yx,
    times=seq(0,35,0.1),
    func="derivs",
    parms=parsx,
    dllname="tm_deb_2",
    initfunc="initmod",
    events=list(data=eventdat)) %>% as.data.frame -> outx
mutate(outx, Wpred=W+E, Lpred=(Wpred/1.8e-3)^(1/3)) -> outx
ode(yy,
    times=seq(0,35,0.1),
    func="derivs",
    parms=parsy,
    dllname="tm_deb_2",
    initfunc="initmod",
    events=list(data=eventdat)) %>% as.data.frame -> outy
mutate(outy, Wpred=W+E, Lpred=(Wpred/1.8e-3)^(1/3)) -> outy
ode(yz,
    times=seq(0,35,0.1),
    func="derivs",
    parms=parsz,
    dllname="tm_deb_2",
    initfunc="initmod",
    events=list(data=eventdat)) %>% as.data.frame -> outz
mutate(outz, Wpred=W+E, Lpred=(Wpred/1.8e-3)^(1/3)) -> outz

## 95% confidence interval around this deterministic prediction, measured at the moments when measurements were actually taken
bootLx <- sapply(outx[unique(data$age)*10+1,"Lpred"], function(m) rnorm(1000, mean=m, sd=parsx["Lobs"]))
bootRx <- sapply(outx[unique(data$age)*10+1,"R"], function(m) rnorm(1000, mean=m, sd=parsx["Robs"]))
bootRx[bootRx < 0] <- 0 ## set all negative observations to be equal to 0

bootLy <- sapply(outy[unique(data$age)*10+1,"Lpred"], function(m) rnorm(1000, mean=m, sd=parsy["Lobs"]))
bootRy <- sapply(outy[unique(data$age)*10+1,"R"], function(m) rnorm(1000, mean=m, sd=m*parsy["Robs"]))
bootRy[bootRy < 0] <- 0 ## set all negative observations to be equal to 0

bootLz <- sapply(outz[unique(data$age)*10+1,"Lpred"], function(m) rnorm(1000, mean=m, sd=parsz["Lobs"]))
bootRz <- sapply(outz[unique(data$age)*10+1,"R"], function(m) rnbinom(1000, mu=m, size=parsz["Robs"]))
bootRz[bootRz < 0] <- 0 ## set all negative observations to be equal to 0

##
par(mfrow=c(3,2), oma=rep(0.5,4), mar=c(5,5,0,0))
plot(outx$time, outx$Lpred, type='l', lwd=3, col="darkgrey", ylim=c(0,2), xlab="Age", ylab="Length (mm)", cex.lab=1.5)
for (i in 1:ncol(bootLx)) lines(rep(unique(data$age)[i],2), sort(bootLx[,i])[c(25,975)], lwd=3)
points(data$age, data$length, col=1, cex=1.5)

plot(outx$time, outx$R, type='l', lwd=3, col="darkgrey", ylim=c(0,60), xlab="Age", ylab="Cumulative eggs", cex.lab=1.5)
for (i in 1:ncol(bootRx)) if(out[unique(data$age)[i]*10+1,"R"] > 0) lines(rep(unique(data$age)[i],2), sort(bootRx[,i])[c(25,975)], lwd=3)
points(data$age, data$eggs, col=1, cex=1.5)

plot(outy$time, outy$Lpred, type='l', lwd=3, col="darkgrey", ylim=c(0,2), xlab="Age", ylab="Length (mm)", cex.lab=1.5)
for (i in 1:ncol(bootLy)) lines(rep(unique(data$age)[i],2), sort(bootLy[,i])[c(25,975)], lwd=3)
points(data$age, data$length, col=1, cex=1.5)

plot(outy$time, outy$R, type='l', lwd=3, col="darkgrey", ylim=c(0,sort(bootRy[,8])[975]), xlab="Age", ylab="Cumulative eggs", cex.lab=1.5)
for (i in 1:ncol(bootRy)) if(out[unique(data$age)[i]*10+1,"R"] > 0) lines(rep(unique(data$age)[i],2), sort(bootRy[,i])[c(25,975)], lwd=3)
points(data$age, data$eggs, col=1, cex=1.5)

plot(outz$time, outz$Lpred, type='l', lwd=3, col="darkgrey", ylim=c(0,2), xlab="Age", ylab="Length (mm)", cex.lab=1.5)
for (i in 1:ncol(bootLz)) lines(rep(unique(data$age)[i],2), sort(bootLz[,i])[c(25,975)], lwd=3)
points(data$age, data$length, col=1, cex=1.5)

plot(outz$time, outz$R, type='l', lwd=3, col="darkgrey", ylim=c(0,sort(bootRz[,8])[975]), xlab="Age", ylab="Cumulative eggs", cex.lab=1.5)
for (i in 1:ncol(bootRz)) if(out[unique(data$age)[i]*10+1,"R"] > 0) lines(rep(unique(data$age)[i],2), sort(bootRz[,i])[c(25,975)], lwd=3)
points(data$age, data$eggs, col=1, cex=1.5)

@

For the model with negative binomial error, if the observation and the prediction are both zero, every value for the size parameter has an identical likelihood (1).

<<echo=FALSE, eval=FALSE>>=
xi <- 1.8e-3; q <- 3;

## which datapoints dominate the likelihood calculation for each model?
pred <- outx
mutate(pred, Wobs=W+E, Lobs=(Wobs/xi)^(1/q)) -> pred
pars <- parsx
sapply(unique(data$age),
       function(d)
           dnorm(x=data$eggs[data$age==d],
                 mean=pred$R[pred$time==d],
                 sd=pars["Robs"],
                 log=TRUE
                 )
       )

pred <- outy
mutate(pred, Wobs=W+E, Lobs=(Wobs/xi)^(1/q)) -> pred
pars <- parsy
sapply(unique(data$age),
       function(d)
           dnorm(x=data$eggs[data$age==d],
                 mean=(pred$R[pred$time==d]+0.001),
                 sd=(pred$R[pred$time==d]+0.001)*pars["Robs"],
                 log=TRUE
                 )
       )

pred <- outz
mutate(pred, Wobs=W+E, Lobs=(Wobs/xi)^(1/q)) -> pred
pars <- parsz
sapply(unique(data$age),
       function(d)
           dnbinom(x=data$eggs[data$age==d],
                   mu=pred$R[pred$time==d],
                   size=pars["Robs"],
                   log=TRUE
                   )
       )

@

Focusing on the model with negative binomial error, Fig. \ref{fig:Cat-fits-1} shows that the range of parameter estimates is very large among parameter sets that had negative log-likelihoods within two units of the minimum.
There are also very strong correlations among parameter estimates, in particular between the fraction of resources allocated towards growth $\kappa$ and the cost of reproduction $E_R$ and between the assimilation efficiency $\rho$ and the size at maturity $W_{mat}$.
This is indicative of parameter unidentifiability with the fitting algorithm.
This is especially clear for the half-saturation constant, which varies between 3 and 144,000, all in parameter sets that differ by less than two log-liklihood units.
This non-identifiability of $F_h$ greatly affects the estimates of other parameters.

<<echo=FALSE, fig.width=5, fig.height=5, fig.cap="Pairwise plot of parameter estimates from fitting Cat's growth and reproduction dataset.", label="Cat-fits-1">>=
ggpairs(z, c(1:5,8))


@

I think the best thing to do is to try fixing $F_h$ at different values, and see how that affects the other parameter estimates, since $F_h$  is specified by the feeding model, which was also ambiguous - relationships between parameters of the fitting model were highly constrained, but the parameter estimates themselves were highly flexible.
Although I doubt if there is a parameter set with a higher likelihood that was overlooked by the algorithm, my sense is that it may be that the likelihood surface is quite flat, so $F_h$ can run off towards 0 or infinity, dragging all of the other parameter estimates with it.
The profile likelihood will be very informative in this regard.

<<echo=FALSE, fig.width=5, fig.height=5, fig.cap="Parameter estimates and likelihoods as the half-saturation constant and mobilization flux are varied. The black dots indicate the parameter estimates with lowest negative log-likelihood.", label="Fh-profile">>=
x10 <- readRDS("Profile_lik_Fh_uninfected_animals_5-11.RDS")

Fhseq <- c(100,500,seq(1000,20000,1000))
y <- lapply(1:length(Fhseq), function(i) subset(x10[[i]], lik < min(lik)+2 & conv==0) %>% mutate(., Fh=Fhseq[i], col=ifelse(lik==min(lik),1,0)) %>% do.call("rbind",.) %>% t) %>% do.call("rbind",.) %>% as.data.frame %>% gather(., "param", "estimate", 1:8) %>% arrange(., col)
y$param <- factor(y$param, levels=c("rho","K","km","ER","Wmat","Lobs","Robs","lik"))

ggplot(subset(y, param%in%c("ER","K","km","rho","Wmat","lik"))) +
    facet_grid(param~., scales="free_y") +
        geom_point(aes(x=Fh, y=estimate, color=as.factor(col))) +
            theme_bw() +
                theme(legend.position='none') +
                    scale_colour_manual(values=c("gray","black"))


@


Fig. \ref{fig:Fh-profile} shows pretty clearly that many of the key parameters are dependent on the value of $F_h$.
Another obvious take-away from this plot is that $\kappa$ and $E_R$ are hard to estimate.
Basically, there is very little information to fix either the value of $F_h$ (or $v$, which I have previously confirmed).
Fortunately, there are plenty of other datasets that have estimated values of $F_h$ that I can use to fix the value of $F_h$ for these parameter estimations.
Nisbet et al. (2004) estimates $F_h = 0.08$ mgC/L for \emph{D. pulicaria}; McCauley et al. (1990) estimates $F_h = 0.16$ mgC/L; Hall et al. (2009) estimates $F_h = 0.1$ mgC/L for \emph{D. dentifera}; Martin et al. (2013) estimates $F_h = 1585$ cells/mL for \emph{D. magna};
Note that $F_h$ is measured here in units of cells/mL as well; the estimate of carbon context per cell is 8.16536$\times 10^{-9}$ mgC/cell.
If we use 0.1 mgC/L as the half-saturation constant (based on Hall et al.), that would be the equivalent of 0.1 mgC/L divided by 8.16536$\times 10^{-9}$ mgC/cell divided by 1000 mL/L = 12250 cells/mL, whereas if we use $0.16$ (based on McCauley et al. 1990, but also used by McCauley et al. 2008 and Johnson et al. 2013), the half-saturation constant would be 19,600 cells/mL.

Comparing the parameter estimates for $F_h = 12000$ and $F_h = 20000$,

\begin{tabular}{l | l | l}
& $F_h = 12000$ & $F_h = 20000$ \\
$\rho$ & 0.152 & 0.119 \\
$\kappa$ & 0.680 & 0.688 \\
$k_m$ & 0.0727 & 0.0534 \\
$E_R$ & $5.5 \times 10^{-5}$ & $4.8 \times 10^{-5}$ \\
$W_{mat}$ & $6.2 \times 10^{-3}$ & $6.3 \times 10^{-3}$ \\
$L_{obs}$ & 0.102 & 0.102 \\
$R_{obs}$ & 32.1 & 32.1 \\
loglik & -20.6 & -20.2
\end{tabular}

There are only very small differences between these two parameter sets, and no difference in the log-likelihood of either, indicating that they both are equally good at explaining the data.
The relationship between these parameter estimates and other published estimates is also useful.
For example, the estimate of $\kappa$ of 0.68 is \emph{very} different from the estimates of Nisbet et al. (2004).
Nisbet et al. (2004) fits a flexible spline to allocation fraction, showing that it drops from 1 to less than 0.2 very quickly.
McCauley et al. (2008) use an estimate of 0.23 (based on Nisbet).
Johnson et al. (2013) have estimates of $\kappa$ that vary quite considerably depending on the details of the model, from almost 0 when all parameters are left free, to between 0.3 and 0.7 when maximum size is constrained by an informative prior.
Martin et al. (2013) estimated $\kappa$ to be 0.678.
Thus, the estimate of $\kappa$ is reasonably well-supported, since Johnson et al. and Martin et al. both fit the standard DEB model to data, whereas Nisbet et al. was using a somewhat more bespoke model.

We can compare the output of the model when $F_h$ was fixed at 12,000 cells/mL to the observed data to see how well we captured the observed growth and reproduction data.
You can see in Fig. \ref{fig:model-data-comp-2} that the model does a very good job of capturing the mean and variance in the observed data.
On the basis of this, I will assume that I can proceed to fitting models that include parasitism.

<<echo=FALSE, eval=FALSE, fig.width=5, fig.height=5, fig.cap="Parameter estimates and likelihoods as the half-saturation constant and mobilization flux are varied.", label="Fh-profile-old">>=
x10 <- readRDS("Profile_lik_Fh_uninfected_animals_5-11.RDS")
x50 <- readRDS("Profile_lik_Fh_uninfected_animals_v=50_5-11.RDS")
x100 <- readRDS("Profile_lik_Fh_uninfected_animals_v=100_5-11.RDS")

x = list(x10,x50,x100)

vseq <- c(10,50,100)
Fhseq <- c(100,500,seq(1000,20000,1000))
y <- lapply(1:length(vseq), function(j) lapply(1:length(Fhseq), function(i) subset(x[[j]][[i]], lik < min(lik)+2 & conv==0) %>% mutate(., Fh=Fhseq[i], v=paste0("v=",vseq[j]), col=ifelse(lik==min(lik),1,0))) %>% do.call("rbind",.)) %>% do.call("rbind",.) %>% gather(., "param", "estimate", 1:8) %>% arrange(., col)
y$param <- factor(y$param, levels=c("rho","K","km","ER","Wmat","Lobs","Robs","lik"))

ggplot(subset(y, param%in%c("ER","K","km","rho","Wmat","lik"))) +
    facet_grid(param~v, scales="free_y") +
        geom_point(aes(x=Fh, y=estimate, color=as.factor(col))) +
            theme_bw() +
                theme(legend.position='none') +
                    scale_colour_manual(values=c("gray","black"))


@

<<echo=FALSE, fig.height=4, fig.width=6, units='in', fig.cap="Comparing model predictions to observed data for the negative binomial error model for reproduction, with $v=10$ and $F_h=12000$.", label="model-data-comp-2">>=
## Load Cat's data
library(deSolve)
xx <- read.csv("Cat_data/uninfected_growth_reproduction.csv")
data <- xx[1:103,1:3]
data$eggs[is.na(data$eggs)] <- 0 ## set reproduction = 0 for all individuals that have not yet matured
## change the name of 'times' to 'age'
colnames(data)[1] <- 'age'

source("Growth_reproduction_trajectory_matching_real_data_2.R")
fixpars <- c(Imax=22500, Fh=12000, g=1.45, v=10, F0=1e6/30)
## Parameter estimates for the 3 different error models
estpars <- subset(y, Fh==12000 & v=="v=10" & col==1)$estimate[1:7]
names(estpars) <- subset(y, Fh==12000 & v=="v=10" & col==1)$param[1:7]
fixpars["Imax"] <- calc_Imax(unname(fixpars["Fh"]))
fixpars["g"] <- calc_g(unname(fixpars["Fh"]))

parorder <- c("Imax","Fh","g","rho","K","km","v","ER","F0","Lobs","Robs","Wmat")
pars <- c(estpars, fixpars)
pars[match(parorder, names(pars))] -> pars

## Set the initial conditions
y <- c(F=unname(pars["F0"]),
        E=0,
        W=unname(pars["ER"]/(1+pars["rho"]/pars["v"])),
        M=0,
        R=0)
y["E"] <- unname(y["W"]*pars["rho"]/pars["v"])

## Feeding
eventdat <- data.frame(var="F",
                       time=1:35,
                       value=unname(pars["F0"]),
                       method=rep(c(rep("add",4),"rep"),7))
## simulate the system
ode(y,
    times=seq(0,35,0.1),
    func="derivs",
    parms=pars,
    dllname="tm_deb_2",
    initfunc="initmod",
    events=list(data=eventdat)) %>% as.data.frame -> out
mutate(out, Wpred=W+E, Lpred=(Wpred/1.8e-3)^(1/3)) -> out

## 95% confidence interval around this deterministic prediction, measured at the moments when measurements were actually taken
bootL <- sapply(out[unique(data$age)*10+1,"Lpred"], function(m) rnorm(1000, mean=m, sd=pars["Lobs"]))
bootR <- sapply(out[unique(data$age)*10+1,"R"], function(m) rnbinom(1000, mu=m, size=pars["Robs"]))
bootR[bootR < 0] <- 0 ## set all negative observations to be equal to 0


##
par(mfrow=c(1,2), oma=rep(0.5,4), mar=c(5,5,0,0))
plot(out$time, out$Lpred, type='l', lwd=3, col="darkgrey", ylim=c(0,2), xlab="Age", ylab="Length (mm)", cex.lab=1.5)
for (i in 1:ncol(bootL)) lines(rep(unique(data$age)[i],2), sort(bootL[,i])[c(25,975)], lwd=3)
points(data$age, data$length, col=1, cex=1.5)

plot(out$time, out$R, type='l', lwd=3, col="darkgrey", ylim=c(0,60), xlab="Age", ylab="Cumulative eggs", cex.lab=1.5)
for (i in 1:ncol(bootR)) if(out[unique(data$age)[i]*10+1,"R"] > 0) lines(rep(unique(data$age)[i],2), sort(bootR[,i])[c(25,975)], lwd=3)
points(data$age, data$eggs, col=1, cex=1.5)


@



\end{document}