\documentclass[12pt,reqno,final,pdftex]{amsart}
%% DO NOT DELETE OR CHANGE THE FOLLOWING TWO LINES!
%% $Revision$
%% $Date$
\usepackage[round,sort,elide]{natbib}
\usepackage{graphicx}
\usepackage{times}
\usepackage{rotating}
\usepackage{subfig}
\usepackage{color}
\newcommand{\aak}[1]{\textcolor{cyan}{#1}}
\newcommand{\mab}[1]{\textcolor{red}{#1}}
\newcommand{\cec}[1]{\textcolor{blue}{#1}}

\setlength{\textwidth}{6.25in}
\setlength{\textheight}{8.75in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{-.35in}
\setlength{\parskip}{.1in}
\setlength{\parindent}{0.3in}

%% cleveref must be last loaded package
\usepackage[sort&compress]{cleveref}
\newcommand{\crefrangeconjunction}{--}
\crefname{figure}{Fig.}{Figs.}
\Crefname{figure}{Fig.}{Figs.}
\crefname{table}{Table}{Tables}
\Crefname{table}{Tab.}{Tables}
\crefname{equation}{Eq.}{Eqs.}
\Crefname{equation}{Eq.}{Eqs.}
\crefname{appendix}{Appendix}{Appendices}
\Crefname{appendix}{Appendix}{Appendices}
\creflabelformat{equation}{#2#1#3}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{corol}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{hyp}[thm]{Hypothesis}
\newtheorem{example}[thm]{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{algorithm}[thm]{Algorithm}
\newtheorem{remark}{Remark}
\renewcommand\thethm{\arabic{thm}}
\renewcommand{\theremark}{}

\numberwithin{equation}{part}
\renewcommand\theequation{\arabic{equation}}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thefigure{\arabic{figure}}
\renewcommand\thetable{\arabic{table}}
\renewcommand\thefootnote{\arabic{footnote}}

\newcommand\scinot[2]{$#1 \times 10^{#2}$}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textsf{#1}}
\newcommand{\dlta}[1]{{\Delta}{#1}}
\newcommand{\Prob}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\Expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[#1\right]}
\newcommand{\dd}[1]{\mathrm{d}{#1}}
\newcommand{\citetpos}[1]{\citeauthor{#1}'s \citeyearpar{#1}}

\begin{document}

<<setup,include=FALSE,cache=F>>=
require(knitr)
opts_chunk$set(
               progress=T,prompt=F,tidy=F,highlight=T,
               warning=F,message=F,error=F,
               results='hide',echo=F,cache=T,
               size='scriptsize',
               fig.path='figure/',fig.lp="fig:",
               fig.align='left',
               fig.show='asis',
               fig.height=4,fig.width=6.83,
               out.width="\\linewidth",
               dpi=150,
               dev=c('png','tiff'),
               dev.args=list(
                 png=list(bg='transparent'),
                 tiff=list(compression='lzw')
                 )
               )

scinot <- function (x, digits = 2, type = c("expression","latex")) {
  type <- match.arg(type)
  x <- signif(x,digits=digits)
  ch <- floor(log10(abs(x)))
  mn <- x/10^ch
  switch(type,
         expression={
           bquote(.(mn)%*%10^.(ch))
         },
         latex={
           paste0("\\scinot{",mn,"}{",ch,"}")
         }
         )
}

require(xtable)

options(scipen=-1)

options(
        xtable.caption.placement="top",
        xtable.include.rownames=FALSE
        )

@

\section*{Changes from previous version}

Previously, the fitting algorithm had significant problems attempting to fit even simulated datasets.
I spent a lot of time trying to figure out why, and discovered a number of problems that I have fixed in the current version of both the model and the fitting algorithm.
Here's a brief list of the changes:
\begin{enumerate}
\item Previously, I assumed a Poisson distribution of measurement error. This is convenient because it is discrete and because it has no parameters to estimate (because the variance is equal to the mean). However, because we are measuring cumulative reproduction, this creates huge problems for fitting. For example, if the model-predicted cumulative reproduction was 50, and the observed cumulative reproduction was 45, I evaluated the likelihood using the call \texttt{dpois(x=45, lambda=50)}, so the measurement error variance was equal to 50! Basically, the Poisson has a variance that is far too high, given that observed reproduction is actually likely to have low measurement error - it's pretty easy to count babies! Now I am assuming normally distributed error.
\item Previously, I showed that it was essentially impossible to estimate the cost of reproduction $E_R$ while simultaneously estimating the allocation to growth $\kappa$ and the assimilation efficiency $\rho$ - the algorithm could arbitrarily slide these parameters around, making them essentially impossible to estimate simultaneously. However, I realized that there is actually some independent information about the cost of reproduction, which is that the cost of reproduction should be related to the initial values of reserves and structural mass. Now I am estimating the cost of reproduction, but assuming that $E_R = E(0) + W(0)$ to help constrain the fitting. I am using a standard DEB assumption to relate $E(0)$ and $W(0)$, which is that, at birth, $E(0)/W(0)$ is equal to the maximum reserve density, which is equal to $\rho/\nu$ (where $\rho$ is the assimilation efficiency and $\nu$ is the ``energy conductance'').
\end{enumerate}

Thus, the model that I am fitting is the following:
\begin{align}
\frac{dF}{dt} &= -I_{max} \frac{F}{F_h+F} L_{obs}^g, \\
\frac{dE}{dt} &= \rho \epsilon V I_{max} \frac{F}{F_h+F} L_{obs}^g - P_C, \\
\frac{dW}{dt} &= \kappa P_C - k_m W, \\
\frac{dR}{dt} &= \frac{(1-\kappa) P_C}{E_R}, \text{ where} \\
P_C &= \frac{E (\nu/L + k_m)}{1 + \kappa E/W}, \\
L &= W^{1/3}, \\
W_{obs} &= W + E, \\
L_{obs} &= (\frac{W_{obs}}{\xi})^{1/q},
F(0) &= F_0, \\
E(0) &= \frac{\rho W(0)}{\nu}, \\
W(0) &= \frac{E_R}{1+\rho/\nu}, \text{ and}\\
R(0) &= 0.
\end{align}

There are several things to note about this model.
Ingestion rate depends on observed length, $L_{obs}$, rather than the DEB variable of ``structural'' length.
This takes advantage of our independent feeding data, which allows us to relate observed length to the feeding parameters $I_{max}$, $F_h$, and $g$.
Somatic maintenance rate depends on the DEB variable of structural weight, $W$, and mobilization flux $P_C$ depends on structural length, $L$, where $W = L^3$.
The relationship between observed length and the DEB variables is based on a length-weight regression.
Since observed weight, $W_{obs}$, should account for weight as structural weight and weight of reserves, we assume that $W_{obs} = W + E$ and $L_{obs} = (W_{obs}/\xi)^{(1/q)}$, where $\xi$ and $q$ are parameters of the length-weight regression.
For simplicity, we assume that the cost of growth $E_G$ is equal to one.
That suggests that reserves and structural weight are measured in biomass units.
This was an assumption that was necessary because it was possible to find parameter sets with identical likelihoods but very different estimates of cost of growth, cost of reproduction ($E_R$), and growth allocation ($\kappa$).
We assume that the cost of growth $E_R$ is equal to the amount of biomass in a neonate, so $E_R = E(0) + W(0)$.
The partitioning between reserves and weight based on the standard DEB assumption that reserve density, $E/W$, is equal to $\rho/\nu$.
Finally, contrary to the standard DEB theory, we do not consider ``maturity maintenance'', so the allocation of biomass to reproduction can never be reclaimed via ``shrinking.''

Of the parameters of this model, many are fixed.
The carbon content of algae, $\epsilon = 8.17 \times 10^{-9}$, is based on dry weight of a known number of \emph{Ankistrodesmus} cells, which were used in the experiment.
The volume of the experimental container is $V = 30$ml.
The parameters of the length-weight regression are $\xi = 1.8 \times 10^{-3}$ and $q = 3$.
The energy conductance parameter $\nu$ is fixed at a value of 10, as previous fitting attempts by ourselves and others (Martin et al. 2013) have shown that this parameter cannot be independently estimated.

Two other parameters are set by the results of the feeding model fitting.
The algorithm estimates the value of $F_h$ (the half-saturation constant), but once the value of $F_h$ is specified, both the maximum ingestion rate $I_{max}$ and the length-feeding rate exponent $g$ are set by the results of the feeding model fitting.
Thus the only parameters that are estimated are $F_h$, $\rho$ (the assimilation efficiency), $\kappa$ (the fractional allocation to growth), $k_m$ (the somatic maintenance rate), $E_R$ (the cost of reproduction), the structural weight at maturation (when reproduction begins), $W_{mat}$, and the standard deviations of normal distributions describing the error in measurement of length and reproduction, $\sigma_L$ and $\sigma_R$.

To perform a simulation/recovery experiment, we generated 25 simulated datasets.
To generate each dataset, we randomly drew values of $F_h$, $\rho$, $\kappa$, $k_m$, $\nu$, $E_R$, and $W_{mat}$ to create a parameter set.
For each parameter set, we simulated growth and reproduction trajectories by allowing the amount of food added each timestep to vary, with a mean of $F(0)=33333$ and a standard deviation of 5000.
The variability in food addition is meant to reflect the reality that food addition is not perfect, even though we assume that it is in the model.
With the standard deviation being so high, this produces a lot of variation in food addition, with large impacts on growth and reproduction.
In reality, the standard deviation in food addition is probably much less than this, so the simulation may be overly pessimistic.
For each of 8 timepoints (day 5, 10, 12, 15, 18, 25, 30, and 35, matching the days that experimental observations of growth and reproduction were collected), we generated 12 independent observations of growth and reproduction, leading to a simulated dataset with 96 datapoints.

You can see in Fig. \ref{fig:rep-cost-est-2} that, overall, the algorithm does a pretty good job estimating all of the parameters.
The estimates are reasonably close to the true values for every parameter and dataset, as the the true value and estimate pairs are fairly closely scattered around the one-to-one line.

<<'rep-cost-est-2', fig.height=4, fig.width=6, fig.cap='The highest likelihood parameter estimate from fitting each of 25 parameter sets using the model with normal measurement errors. Note that here I am also estimating the cost of reproduction. The true parameter value is given by the x-axis value and the estimate is given by the y-axis value. The line is the one-to-one line for reference.'>>=
datasets <- readRDS("Trajectory_matching_datasets_11-22.RDS")
results <- readRDS("Trajectory_matching_estimates_11-22.RDS")

library(dplyr)
library(tidyr)
library(ggplot2)

cbind(lapply(datasets, function(l) l$params[colnames(results[[1]])[1:8]]) %>%
          unlist %>%
              matrix(., ncol=8, byrow=TRUE, dimnames=list(as.character(1:25),colnames(results[[1]])[1:8])) %>%
                  as.data.frame %>%
                      mutate(., Fh=log(Fh)) %>%
                          gather(., key="parameter", value="truth", 1:8),
      lapply(results, function(l) head(l,1)[1:8]) %>%
          unlist %>%
              matrix(., ncol=8, byrow=TRUE, dimnames=list(as.character(1:25),colnames(results[[1]])[1:8])) %>%
                  as.data.frame %>%
                      mutate(., Fh=log(Fh)) %>%
                          gather(., key="parameter", value="estimate", 1:8)
      ) -> best
## Remove extra "parameter" column
best <- best[,-3]

ggplot(best[which(best[,1] %in% c("Fh","rho","K","km","ER","Wmat")),], aes(x=truth, y=estimate)) +
    geom_point() +
        facet_wrap(~parameter, scales='free') +
            geom_abline(slope=1, intercept=0)

@

Fig. \ref{fig:rel-error-25-2} shows the relative (fractional) error for each parameter, organized by dataset.
Again, these suggest that error was fairly reasonable for most of the datasets.
One of the few datasets that was difficult to estimate was dataset 5.

<<'rel-error-25-2', fig.height=4, fig.width=6, fig.cap='Relative error in the parameter estimates for all 25 datasets.'>>=
mutate(best, error=(estimate-truth)/truth) -> best
best$set <- rep(1:25, 8)

ggplot(best, aes(x=set, y=error)) +
    geom_point() +
        facet_wrap(~parameter, scales="free")

@

\clearpage

However, if you look at the growth and reproduction trajectories for dataset 5 compared to all of the other growth and reproduction trajectories, you can see that this dataset had the lowest reproduction (the red line in Fig. \ref{fig:comparing-trajectories}).
Another dataset that was somewhat challenging to estimate was dataset 10, which also had low reproduction (the blue line).
Note that the two datasets with very high reproduction were datasets 6 and 19, which have very high estimates of the observation error in reproduction, but the other parameters were well-estimated.

<<'comparing-trajectories', fig.width=6, fig.height=4, fig.cap='The growth and reproduction trajectories at the true parameter values for dataset 13 (black) and all other datasets (grey).'>>=
## what is the likelihood of the true parameter values?
source("Growth_reproduction_trajectory_matching.R")

out <- vector(mode='list', length=25)
for (i in 1:25) {
    estpars <- results[[i]][1,1:8] %>% unlist
    fixpars <- c(Imax=22500, g=1.45, v=10, F0=1e6/30)
    parorder <- c("Imax","Fh","g","rho","K","km","v","ER","F0","Lobs","Robs","Wmat")
    fixpars["Imax"] <- calc_Imax(unname(estpars["Fh"]))
    fixpars["g"] <- calc_g(unname(estpars["Fh"]))
    ## combine the parameters to be estimated and the fixed parameters
    ## into a single vector, with an order specified by parorder
    pars <- c(estpars, fixpars)
    pars[match(parorder, names(pars))] -> pars

    eventdat <- data.frame(var="F",
                           time=1:35,
                           value=unname(pars["F0"]),
                           method=rep(c(rep("add",4),"rep"),35/5))

    ## Initial condition
    y0 <- c(F=unname(pars["F0"]),
            E=0,
            W=unname(pars["ER"]/(1+pars["rho"]/pars["v"])),
            R=0)
    y0["E"] <- unname(y0["W"]*pars["rho"]/pars["v"])

    ## Simulate the system
    try(ode(y0,
            times=0:35,
            func="derivs",
            parms=pars,
            dllname="tm_deb",
            initfunc="initmod",
            events=list(data=eventdat))) %>%
                as.data.frame %>%
                    mutate(., L=((W+E)/0.0018)^(1/3)) -> out[[i]]

}

par(mfrow=c(1,2), mar=c(4,4,0.25,0.25), oma=rep(0.25,4))
plot.new()
plot.window(xlim=c(0,35), ylim=c(lapply(out, function(o) min(o$L)) %>% unlist %>% min, lapply(out, function(o) max(o$L)) %>% unlist %>% max))
box('plot')
axis(1)
axis(2)
mtext(side=1, line=2.5, 'Age')
mtext(side=2, line=2.5, 'Length (mm)')
for (i in 1:25) with(out[[i]], lines(time, L, col=grey(0.7)))
with(out[[5]], lines(time, L, col=2, lwd=2))
with(out[[10]], lines(time, L, col=4, lwd=2))

plot.new()
plot.window(xlim=c(0,35), ylim=c(lapply(out, function(o) min(o$R)) %>% unlist %>% min, lapply(out, function(o) max(o$R)) %>% unlist %>% max))
box('plot')
axis(1)
axis(2)
mtext(side=1, line=2.5, 'Age')
mtext(side=2, line=2.5, 'Cum. no. of eggs')
for (i in 1:25) with(out[[i]], lines(time, R, col=grey(0.7)))
with(out[[5]], lines(time, R, col=2, lwd=2))
with(out[[10]], lines(time, R, col=4, lwd=2))

@


\clearpage

I am wondering, however, whether I couldn't do better if I increased the size of the initial box of guesses and the number of estimates that I refine through Nelder-Mead.
In particular, I am wondering whether some parameter sets that are somewhat close to the true parameter set have lower likelihoods than other places in parameter space, and are thus excluded from the Nelder-Mead optimization, even though if they were optimized they would end up with higher likelihoods.
Another possibility, of course, is that the true parameter values are actually somewhat less likely than other places and, moreover, sit on a very steep, isolated peak in likelihood space, so that they are hard to find and hard to stay near.
I am going to explore this by initializing optimizations at the true parameter values and then seeing how the results of those optimizations compare with the results from the blind optimization.

Fig. \ref{fig:free-vs-truth-start} shows that, when you start the optimization algorithm at the true parameter values, the ML parameter estimates are closer to the truth (in almost every case) than those started blindly.

<<'free-vs-truth-start', fig.width=6, fig.height=4, fig.cap='Comparing the true parameter values (x-axis) against the parameter estimates when the optimization algorithm starts from naive guesses and when the algorithm starts at the true parameter values.'>>=
source("Growth_reproduction_trajectory_matching.R")
datasets <- readRDS("Trajectory_matching_datasets_8-9.RDS")

if (!file.exists("Trajectory_matching_estimates_starting_at_truth.RDS")) {
    results <- vector(mode='list', length=25)
    for (i in 1:25){
        print(i)
        optimizer(estpars=datasets[[i]]$params[c("Fh","rho","K","km","ER","Lobs","Robs")],
                  fixpars=datasets[[i]]$params[c("Imax","g","v","F0")],
                  parorder=c("Imax","Fh","g","rho","K","km","v","ER","F0","Lobs","Robs"),
                  transform=c("log","logit","logit",rep("log",4)),
                  obsdata=datasets[[i]]$data,
                  type="trajectory_matching",
              method="Nelder-Mead") -> results[[i]]
    }
    saveRDS(results, file="Trajectory_matching_estimates_starting_at_truth.RDS")
} else readRDS(file="Trajectory_matching_estimates_starting_at_truth.RDS") -> results

truth <- vector(mode='list', length=25)
for (i in 1:25){
    print(i)
    optimizer(estpars=datasets[[i]]$params[c("Fh","rho","K","km","ER","Lobs","Robs")],
              fixpars=datasets[[i]]$params[c("Imax","g","v","F0")],
              parorder=c("Imax","Fh","g","rho","K","km","v","ER","F0","Lobs","Robs"),
              transform=c("log","logit","logit",rep("log",4)),
              obsdata=datasets[[i]]$data,
              eval.only=TRUE,
              type="trajectory_matching",
              method="Nelder-Mead") -> truth[[i]]
}

estimates <- readRDS("Trajectory_matching_estimates_8-9.RDS")

data.frame(parameter=c("Fh","rho","K","km","ER","Lobs","Robs","lik"),
           set=rep(1:25,each=8),
           optimFree=lapply(estimates, function(e) e[1,1:8]) %>% unlist,
           optimTruth=lapply(results, function(r) c(r$params, r$lik)) %>% unlist,
           truth=lapply(truth, function(t) c(t$params, t$lik)) %>% unlist
           ) %>%
    gather(., key="likMethod", value="value", 3:4) -> out

ggplot(subset(out, parameter%in%c("Fh","rho","K","km","ER")), aes(x=truth, y=value, color=likMethod)) +
    geom_abline(slope=1, intercept=0) +
        geom_point() +
            facet_wrap(~parameter, scales="free")


@

More interesting, however, are the results of the likelihood calculation.
The likelihoods of the parameter estimates found either by the full optimization process or by starting at the true parameter values are so close that they lie on top of one another for almost every dataset (Fig. \ref{fig:lik-comparison}).
However, the likelihoods of the true parameter sets themselves are often much, much smaller.
This is most evident for parameter set 13, but you can see huge log-likelihood differences across the board.
However, it is also important to note that, in all but one case, the likelihood found by the full optimization is at least as good as the likelihood found by starting at the true parameter values (Fig. \ref{fig:lik-comparison-2}), although the differences are almost always very tiny.

<<'lik-comparison', fig.height=3, fig.width=4, fig.cap='Comparing the likelihood of the true parameter values to the likelihoods obtained by the fitting algorithm when initialized completely randomly and when initialized at the true parameter values.'>>=

data.frame(parameter=c("Fh","rho","K","km","ER","Lobs","Robs","lik"),
           set=rep(1:25,each=8),
           optimFree=lapply(estimates, function(e) e[1,1:8]) %>% unlist,
           optimTruth=lapply(results, function(r) c(r$params, r$lik)) %>% unlist,
           truth=lapply(truth, function(t) c(t$params, t$lik)) %>% unlist
           ) %>%
    gather(., key="likMethod", value="value", 3:5) %>%
        subset(., parameter=='lik')-> out2

ggplot(out2, aes(x=set, y=log(value), color=likMethod)) +
    geom_point() +
        xlab("Dataset") +
            ylab("Log-likelihood")


@

<<'lik-comparison-2', fig.height=3, fig.width=4, fig.cap='Comparing the log-likelihoods found via the optimization algorithm starting without any constraint versus that found from starting at the true parameter values. Red points indicate datasets where the likelihood was lower for the free start.'>>=
newd = data.frame(x=out2$value[out2$likMethod=='optimTruth'], y=out2$value[out2$likMethod=='optimFree'])
mutate(newd, col=factor(ifelse(y<x, 1, 0))) -> newd

ggplot(newd, aes(x=x, y=y, col=col)) +
        geom_abline(slope=1, intercept=0) +
            geom_point() +
                xlab("Log-likelihood, starting at the truth") +
                    ylab("Log-likelihood, starting randomly") +
                        scale_colour_manual(values=c('black','red')) +
                                theme_bw() +
                                    theme(legend.position='none')

@

\clearpage

Thus it appears to be generally true that there will be multiple parameter sets that give rise to nearly identical likelihoods, making it impossible to know, in general, which is closer to the truth.
However, the overall message from this fitting is pretty encouraging, as most of the parameter estimates are close to the truth and the algorithm is finding high likelihood parameter sets.
The problem of ridges in likelihood space is almost certain to be unavoidable.

Moreover, if I reduce the variability in the food addition (from a standard deviation of 15000 to just 5000, which is probably more realistic), the estimates improve further.
Fig. \ref{fig:rep-cost-est-3} shows the relative error in the parameter estimates for 25 parameter sets (note that all parameters are equal to the parameter sets above except for the variability in food addition).
As you can see, there is only one dataset with absolute relative error greater than one (dataset 13, which hugely overestimated the cost of reproduction and the observation error).
However, this is the parameter set that produced an individual that reproduced far more than any other, so this result is expected.
Otherwise, the parameter estimates look good for basically every parameter.

<<'rep-cost-est-3', fig.height=4, fig.width=6, fig.cap='Relative error in parameter estimates for 25 parameter sets when the variability in food addition is reduced.'>>=

datasets <- readRDS("Trajectory_matching_datasets_8-23.RDS")
results <- readRDS("Trajectory_matching_estimates_8-23.RDS")

library(dplyr)
library(tidyr)
library(ggplot2)

cbind(lapply(datasets, function(l) l$params[colnames(results[[1]])[1:7]]) %>%
          unlist %>%
              matrix(., ncol=7, byrow=TRUE, dimnames=list(as.character(1:25),colnames(results[[1]])[1:7])) %>%
                  as.data.frame %>%
                      mutate(., Fh=log(Fh)) %>%
                          gather(., key="parameter", value="truth", 1:7),
      lapply(results, function(l) head(l,1)[1:7]) %>%
          unlist %>%
              matrix(., ncol=7, byrow=TRUE, dimnames=list(as.character(1:25),colnames(results[[1]])[1:7])) %>%
                  as.data.frame %>%
                      mutate(., Fh=log(Fh)) %>%
                          gather(., key="parameter", value="estimate", 1:7)
      ) -> best
best <- best[,-3]

mutate(best, error=(estimate-truth)/truth) -> best
best$set <- rep(1:25, 7)

ggplot(best, aes(x=set, y=error)) +
    geom_point() +
        facet_wrap(~parameter, scales="free")

@

\end{document}