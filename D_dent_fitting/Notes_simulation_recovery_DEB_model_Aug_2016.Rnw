\documentclass[12pt,reqno,final,pdftex]{amsart}
%% DO NOT DELETE OR CHANGE THE FOLLOWING TWO LINES!
%% $Revision$
%% $Date$
\usepackage[round,sort,elide]{natbib}
\usepackage{graphicx}
\usepackage{times}
\usepackage{rotating}
\usepackage{subfig}
\usepackage{color}
\newcommand{\aak}[1]{\textcolor{cyan}{#1}}
\newcommand{\mab}[1]{\textcolor{red}{#1}}
\newcommand{\cec}[1]{\textcolor{blue}{#1}}

\setlength{\textwidth}{6.25in}
\setlength{\textheight}{8.75in}
\setlength{\evensidemargin}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{-.35in}
\setlength{\parskip}{.1in}
\setlength{\parindent}{0.3in}

%% cleveref must be last loaded package
\usepackage[sort&compress]{cleveref}
\newcommand{\crefrangeconjunction}{--}
\crefname{figure}{Fig.}{Figs.}
\Crefname{figure}{Fig.}{Figs.}
\crefname{table}{Table}{Tables}
\Crefname{table}{Tab.}{Tables}
\crefname{equation}{Eq.}{Eqs.}
\Crefname{equation}{Eq.}{Eqs.}
\crefname{appendix}{Appendix}{Appendices}
\Crefname{appendix}{Appendix}{Appendices}
\creflabelformat{equation}{#2#1#3}

\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{corol}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{hyp}[thm]{Hypothesis}
\newtheorem{example}[thm]{Example}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{algorithm}[thm]{Algorithm}
\newtheorem{remark}{Remark}
\renewcommand\thethm{\arabic{thm}}
\renewcommand{\theremark}{}

\numberwithin{equation}{part}
\renewcommand\theequation{\arabic{equation}}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thefigure{\arabic{figure}}
\renewcommand\thetable{\arabic{table}}
\renewcommand\thefootnote{\arabic{footnote}}

\newcommand\scinot[2]{$#1 \times 10^{#2}$}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\pkg}[1]{\textsf{#1}}
\newcommand{\dlta}[1]{{\Delta}{#1}}
\newcommand{\Prob}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\Expect}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[#1\right]}
\newcommand{\dd}[1]{\mathrm{d}{#1}}
\newcommand{\citetpos}[1]{\citeauthor{#1}'s \citeyearpar{#1}}

\begin{document}

<<setup,include=FALSE,cache=F>>=
require(knitr)
opts_chunk$set(
               progress=T,prompt=F,tidy=F,highlight=T,
               warning=F,message=F,error=F,
               results='hide',echo=F,cache=T,
               size='scriptsize',
               fig.path='figure/',fig.lp="fig:",
               fig.align='left',
               fig.show='asis',
               fig.height=4,fig.width=6.83,
               out.width="\\linewidth",
               dpi=150,
               dev=c('png','tiff'),
               dev.args=list(
                 png=list(bg='transparent'),
                 tiff=list(compression='lzw')
                 )
               )

scinot <- function (x, digits = 2, type = c("expression","latex")) {
  type <- match.arg(type)
  x <- signif(x,digits=digits)
  ch <- floor(log10(abs(x)))
  mn <- x/10^ch
  switch(type,
         expression={
           bquote(.(mn)%*%10^.(ch))
         },
         latex={
           paste0("\\scinot{",mn,"}{",ch,"}")
         }
         )
}

require(xtable)

options(scipen=-1)

options(
        xtable.caption.placement="top",
        xtable.include.rownames=FALSE
        )

@

\section*{Changes from previous version}

Previously, the fitting algorithm had significant problems attempting to fit even simulated datasets.
I spent a lot of time trying to figure out why, and discovered a number of problems that I have fixed in the current version of both the model and the fitting algorithm.
Here's a brief list of the changes:
\begin{enumerate}
\item Previously, I assumed a Poisson distribution of measurement error. This is convenient because it is discrete and because it has no parameters to estimate (because the variance is equal to the mean). However, because we are measuring cumulative reproduction, this creates huge problems for fitting. For example, if the model-predicted cumulative reproduction was 50, and the observed cumulative reproduction was 45, I evaluated the likelihood using the call \texttt{dpois(x=45, lambda=50)}, so the measurement error variance was equal to 50! Basically, the Poisson has a variance that is far too high, given that observed reproduction is actually likely to have low measurement error - it's pretty easy to count babies! Now I am assuming normally distributed error.
\item Previously, I showed that it was essentially impossible to estimate the cost of reproduction $E_R$ while simultaneously estimating the allocation to growth $\kappa$ and the assimilation efficiency $\rho$ - the algorithm could arbitrarily slide these parameters around, making them essentially impossible to estimate simultaneously. However, I realized that there is actually some independent information about the cost of reproduction, which is that the cost of reproduction should be related to the initial values of reserves and structural mass. Now I am estimating the cost of reproduction, but assuming that $E_R = E(0) + W(0)$ to help constrain the fitting. I am using a standard DEB assumption to relate $E(0)$ and $W(0)$, which is that, at birth, $E(0)/W(0)$ is equal to the maximum reserve density, which is equal to $\rho/\nu$ (where $\rho$ is the assimilation efficiency and $\nu$ is the ``energy conductance'').
\end{enumerate}

Thus, the model that I am fitting is the following:
\begin{align}
\frac{dF}{dt} &= -I_{max} \frac{F}{F_h+F} L_{obs}^g, \\
\frac{dE}{dt} &= \rho \epsilon V I_{max} \frac{F}{F_h+F} L_{obs}^g - P_C, \\
\frac{dW}{dt} &= \kappa P_C - k_m W, \\
\frac{dR}{dt} &= \frac{(1-\kappa) P_C}{E_R}, \text{ where} \\
P_C &= \frac{E (\nu/L + k_m)}{1 + \kappa E/W}, \\
L &= W^{1/3}, \\
W_{obs} &= W + E, \\
L_{obs} &= (\frac{W_{obs}}{\xi})^{1/q},
F(0) &= F_0, \\
E(0) &= \frac{\rho W(0)}{\nu}, \\
W(0) &= \frac{E_R}{1+\rho/\nu}, \text{ and}\\
R(0) &= 0.
\end{align}

There are several things to note about this model.
Ingestion rate depends on observed length, $L_{obs}$, rather than the DEB variable of ``structural'' length.
This takes advantage of our independent feeding data, which allows us to relate observed length to the feeding parameters $I_{max}$, $F_h$, and $g$.
Somatic maintenance rate depends on the DEB variable of structural weight, $W$, and mobilization flux $P_C$ depends on structural length, $L$, where $W = L^3$.
The relationship between observed length and the DEB variables is based on a length-weight regression.
Since observed weight, $W_{obs}$, should account for weight as structural weight and weight of reserves, we assume that $W_{obs} = W + E$ and $L_{obs} = (W_{obs}/\xi)^{(1/q)}$, where $\xi$ and $q$ are parameters of the length-weight regression.
For simplicity, we assume that the cost of growth $E_G$ is equal to one.
That suggests that reserves and structural weight are measured in biomass units.
This was an assumption that was necessary because it was possible to find parameter sets with identical likelihoods but very different estimates of cost of growth, cost of reproduction ($E_R$), and growth allocation ($\kappa$).
We assume that the cost of growth $E_R$ is equal to the amount of biomass in a neonate, so $E_R = E(0) + W(0)$.
The partitioning between reserves and weight based on the standard DEB assumption that reserve density, $E/W$, is equal to $\rho/\nu$.
Finally, contrary to the standard DEB theory, we do not consider ``maturity maintenance'', so the allocation of biomass to reproduction can never be reclaimed via ``shrinking.''

Of the parameters of this model, many are fixed.
The carbon content of algae, $\epsilon = 8.17 \times 10^{-9}$, is based on dry weight of a known number of \emph{Ankistrodesmus} cells, which were used in the experiment.
The volume of the experimental container is $V = 30$ml.
The parameters of the length-weight regression are $\xi = 1.8 \times 10^{-3}$ and $q = 3$.
The energy conductance parameter $\nu$ is fixed at a value of 10, as previous fitting attempts by ourselves and others (Martin et al. 2013) have shown that this parameter cannot be independently estimated.

Two other parameters are set by the results of the feeding model fitting.
The algorithm estimates the value of $F_h$ (the half-saturation constant), but once the value of $F_h$ is specified, both the maximum ingestion rate $I_{max}$ and the length-feeding rate exponent $g$ are set by the results of the feeding model fitting.
Thus the only parameters that are estimated are $F_h$, $\rho$ (the assimilation efficiency), $\kappa$ (the fractional allocation to growth), $k_m$ (the somatic maintenance rate), $E_R$ (the cost of reproduction), the structural weight at maturation (when reproduction begins), $W_{mat}$, and the standard deviations of normal distributions describing the error in measurement of length and reproduction, $\sigma_L$ and $\sigma_R$.

To perform a simulation/recovery experiment, we generated 25 simulated datasets.
To generate each dataset, we randomly drew values of $F_h$, $\rho$, $\kappa$, $k_m$, $\nu$, $E_R$, and $W_{mat}$ to create a parameter set.
For each parameter set, we simulated growth and reproduction trajectories by allowing the amount of food added each timestep to vary, with a mean of $F(0)=33333$ and a standard deviation of 5000.
The variability in food addition is meant to reflect the reality that food addition is not perfect, even though we assume that it is in the model.
With the standard deviation being so high, this produces a lot of variation in food addition, with large impacts on growth and reproduction.
In reality, the standard deviation in food addition is probably much less than this, so the simulation may be overly pessimistic.
For each of 8 timepoints (day 5, 10, 12, 15, 18, 25, 30, and 35, matching the days that experimental observations of growth and reproduction were collected), we generated 12 independent observations of growth and reproduction, leading to a simulated dataset with 96 datapoints.

You can see in Fig. \ref{fig:rep-cost-est-2} that, overall, the algorithm does a pretty good job estimating all of the parameters.
The estimates are very close to the true values for almost every parameter and every dataset, as the the true value and estimate pairs are closely scattered around the one-to-one line.
In fact, there were only a few datasets where the algorithm really failed to estimate parameter reasonably well.

<<'rep-cost-est-2', fig.height=4, fig.width=6, fig.cap='The highest likelihood parameter estimate from fitting each of 25 parameter sets using the model with normal measurement errors. Note that here I am also estimating the cost of reproduction. The true parameter value is given by the x-axis value and the estimate is given by the y-axis value. The line is the one-to-one line for reference.'>>=
datasets <- readRDS("Trajectory_matching_datasets_8-9.RDS")
results <- readRDS("Trajectory_matching_estimates_8-9.RDS")

library(dplyr)
library(tidyr)
library(ggplot2)

cbind(lapply(datasets, function(l) l$params[colnames(results[[1]])[1:7]]) %>%
          unlist %>%
              matrix(., ncol=7, byrow=TRUE, dimnames=list(as.character(1:25),colnames(results[[1]])[1:7])) %>%
                  as.data.frame %>%
                      mutate(., Fh=log(Fh)) %>%
                          gather(., key="parameter", value="truth", 1:7),
      lapply(results, function(l) head(l,1)[1:7]) %>%
          unlist %>%
              matrix(., ncol=7, byrow=TRUE, dimnames=list(as.character(1:25),colnames(results[[1]])[1:7])) %>%
                  as.data.frame %>%
                      mutate(., Fh=log(Fh)) %>%
                          gather(., key="parameter", value="estimate", 1:7)
      ) -> best
best <- best[,-3]

ggplot(best[which(best[,1] %in% c("Fh","rho","K","km","ER")),], aes(x=truth, y=estimate)) +
    geom_point() +
        facet_wrap(~parameter, scales='free') +
            geom_abline(slope=1, intercept=0)



@

Fig. \ref{fig:rel-error-25-2} shows the relative (fractional) error for each parameter, organized by dataset.
For some of these datasets, we can understand what was going on that caused the parameter estimate to be so badly off.
For example, dataset 13 produced an estimate of assimilation efficiency that was much too high (the true value was 0.27 and the estimate was 0.99), an estimate of cost of reproduction that was too high (the true value was 5.2e-3 and the etimate was 2.4e-4), and an estimate of the observation error in reproduction that was much too high (the true value was 2 and the estimate was 134).

<<'rel-error-25-2', fig.height=4, fig.width=6, fig.cap='Relative error in the parameter estimates for all 25 datasets.'>>=
mutate(best, error=(estimate-truth)/truth) -> best
best$set <- rep(1:25, 7)

ggplot(best, aes(x=set, y=error)) +
    geom_point() +
        facet_wrap(~parameter, scales="free")

@

\clearpage

However, if you look at the growth and reproduction trajectories for dataset 13 compared to all of the other growth and reproduction trajectories, you can see the problem.
Fig. \ref{fig:comparing-trajectories} shows that dataset 13 produced an animal that grew incredibly quickly and produced a \emph{huge} number of offspring.
This was because the true value of the cost of reproduction (which also sets the initial size of the offspring) was very low compared to all of the other parameter sets.
With such an extreme parameter value, the dataset itself was also extreme, and the algorithm struggled to find a good estimate.

<<'comparing-trajectories', fig.width=6, fig.height=4, fig.cap='The growth and reproduction trajectories at the true parameter values for dataset 13 (black) and all other datasets (grey).'>>=
## what is the likelihood of the true parameter values?
source("Growth_reproduction_trajectory_matching.R")

out <- vector(mode='list', length=25)
for (i in 1:25) {
    estpars <- results[[i]][1,1:7] %>% unlist
    fixpars <- c(Imax=22500, g=1.45, v=10, F0=1e6/30)
    parorder <- c("Imax","Fh","g","rho","K","km","v","ER","F0","Lobs","Robs")
    fixpars["Imax"] <- calc_Imax(unname(estpars["Fh"]))
    fixpars["g"] <- calc_g(unname(estpars["Fh"]))
    ## combine the parameters to be estimated and the fixed parameters
    ## into a single vector, with an order specified by parorder
    pars <- c(estpars, fixpars)
    pars[match(parorder, names(pars))] -> pars

    eventdat <- data.frame(var="F",
                           time=1:35,
                           value=unname(pars["F0"]),
                           method=rep(c(rep("add",4),"rep"),35/5))

    ## Initial condition
    y0 <- c(F=unname(pars["F0"]),
            E=0,
            W=unname(pars["ER"]/(1+pars["rho"]/pars["v"])),
            R=0)
    y0["E"] <- unname(y0["W"]*pars["rho"]/pars["v"])

    ## Simulate the system
    try(ode(y0,
            times=0:35,
            func="derivs",
            parms=pars,
            dllname="tm_deb",
            initfunc="initmod",
            events=list(data=eventdat))) %>%
                as.data.frame %>%
                    mutate(., L=(W/0.00262)^(1/2.4)) -> out[[i]]

}

par(mfrow=c(1,2), mar=c(4,4,0.25,0.25), oma=rep(0.25,4))
plot.new()
plot.window(xlim=c(0,35), ylim=c(lapply(out, function(o) min(o$L)) %>% unlist %>% min, lapply(out, function(o) max(o$L)) %>% unlist %>% max))
box('plot')
axis(1)
axis(2)
mtext(side=1, line=2.5, 'Age')
mtext(side=2, line=2.5, 'Length (mm)')
for (i in 1:25) with(out[[i]], lines(time, L, col=grey(0.7)))
with(out[[13]], lines(time, L, col=2, lwd=2))

plot.new()
plot.window(xlim=c(0,35), ylim=c(lapply(out, function(o) min(o$R)) %>% unlist %>% min, lapply(out, function(o) max(o$R)) %>% unlist %>% max))
box('plot')
axis(1)
axis(2)
mtext(side=1, line=2.5, 'Age')
mtext(side=2, line=2.5, 'Cum. no. of eggs')
for (i in 1:25) with(out[[i]], lines(time, R, col=grey(0.7)))
with(out[[13]], lines(time, R, col=2, lwd=2))

@

Another dataset that provided at least one very badly estimated parameter was dataset 2, where the half-saturation constant was estimated to be 11.7 when the true value was 10285.6.
Although it wasn't quite as large of an error, the estimate of assimilation efficiency was also quite off (the true value was 0.15 and the estimate was 0.26).
Looking at the growth and reproduction trajectories for dataset 2 compared to all of the other datasets (except 13, which we exclude to help with visualization, Fig. \ref{fig:comparing-trajectories-3}), you can see that this dataset was extreme in the other direction - the trajectory produced an animal that was tiny compared to every other animal in the dataset.

<<'comparing-trajectories-2', fig.width=6, fig.height=4, fig.cap='The growth and reproduction trajectories at the true parameter values for dataset 2 (red) and all other parameter sets (grey).'>>=
par(mfrow=c(1,2), mar=c(4,4,0.25,0.25), oma=rep(0.25,4))
plot.new()
plot.window(xlim=c(0,35), ylim=c(lapply(out, function(o) min(o$L)) %>% unlist %>% min, lapply(out, function(o) max(o$L)) %>% unlist %>% max))
box('plot')
axis(1)
axis(2)
mtext(side=1, line=2.5, 'Age')
mtext(side=2, line=2.5, 'Length (mm)')
for (i in c(1:12,14:25)) with(out[[i]], lines(time, L, col=grey(0.7)))
with(out[[2]], lines(time, L, col=2, lwd=2))

plot.new()
plot.window(xlim=c(0,35), ylim=c(lapply(out, function(o) min(o$R)) %>% unlist %>% min, 300))
box('plot')
axis(1)
axis(2)
mtext(side=1, line=2.5, 'Age')
mtext(side=2, line=2.5, 'Cum. no. of eggs')
for (i in c(1:12,14:25)) with(out[[i]], lines(time, R, col=grey(0.7)))
with(out[[2]], lines(time, R, col=2, lwd=2))

@

The final dataset that produced some bad parameter estimates was dataset 11, which produced an estimate of growth allocation $\kappa$ that was too high (the true value was 0.25 and the estimate was 0.93), you can see that again, the dataset was extreme, with the animal growing very rapidly to very large size and having very few offspring (Fig. \ref{fig:comparing-trajectories-2}).

<<'comparing-trajectories-3', fig.width=6, fig.height=4, fig.cap='The growth and reproduction trajectories at the true parameter values for dataset 11 (red) and all other datasets (grey).'>>=
par(mfrow=c(1,2), mar=c(4,4,0.25,0.25), oma=rep(0.25,4))
plot.new()
plot.window(xlim=c(0,35), ylim=c(lapply(out, function(o) min(o$L)) %>% unlist %>% min, lapply(out, function(o) max(o$L)) %>% unlist %>% max))
box('plot')
axis(1)
axis(2)
mtext(side=1, line=2.5, 'Age')
mtext(side=2, line=2.5, 'Length (mm)')
for (i in c(1:12,14:25)) with(out[[i]], lines(time, L, col=grey(0.7)))
with(out[[11]], lines(time, L, col=2, lwd=2))

plot.new()
plot.window(xlim=c(0,35), ylim=c(lapply(out, function(o) min(o$R)) %>% unlist %>% min, 300))
box('plot')
axis(1)
axis(2)
mtext(side=1, line=2.5, 'Age')
mtext(side=2, line=2.5, 'Cum. no. of eggs')
for (i in c(1:12,14:25)) with(out[[i]], lines(time, R, col=grey(0.7)))
with(out[[11]], lines(time, R, col=2, lwd=2))

@

So, basically, the algorithm struggles to fit extreme datasets.
Interestingly, while this datasets are extreme, relative to the other datasets being fit, it is unclear why they should be hard to fit.
An examination of the true parameter values for each dataset can be somewhat illustrative (Fig. \ref{fig:comparing-parameters}).
In particular, as noted above, for the case of dataset 13, one of the parameter values was actually fairly extreme (cost of reproduction was very low).
For dataset 2, on the other hand, both the growth allocation $\kappa$ and the assimilation efficiency $\rho$ were very low.
Dataset 11, on the other hand, doesn't appear to be very unusual in terms of its parameter values.

<<'comparing-parameters', fig.width=6, fig.height=4, fig.cap='True parameter values for all 25 parameter sets. The hard-to-fit parameter sets are colored blue.'>>=
lapply(datasets, function(d) d$params[c("Fh","rho","K","km","ER")]) %>% unlist %>% matrix(., ncol=5, byrow=TRUE, dimnames=list(1:25, c("Fh","rho","K","km","ER"))) %>% as.data.frame %>% mutate(., Fh=log(Fh), set=1:25) %>% gather(., key="parameter", value="value", 1:5) -> truth
truth$col <- 1
truth[truth$set%in%c(2,11,13),'col'] <- 2

ggplot(truth, aes(x=set, y=value, col=factor(col))) +
    geom_point() +
        facet_wrap(~parameter, scales="free") +
            theme_bw() +
                theme(legend.position='none')

@

\clearpage

Regardless, if you remove these three extreme datasets, the plots of the estimates compared to the truth look great (Fig. \ref{fig:best-estimates}).
<<'best-estimates', fig.width=6, fig.height=4, fig.cap="Looking at the estimates when you exclude the 3 datasets that were hard to fit.">>=
ggplot(subset(best, parameter%in%c("Fh","rho","K","km","ER") & set%in%c(1,3:10,12,14:25)), aes(x=truth, y=estimate)) +
    geom_point() +
        facet_wrap(~parameter, scales='free') +
            geom_abline(slope=1, intercept=0)


@

\clearpage

I am wondering, however, whether I couldn't do better if I increased the size of the initial box of guesses and the number of estimates that I refine through Nelder-Mead.
In particular, I am wondering whether some parameter sets that are somewhat close to the true parameter set have lower likelihoods than other places in parameter space, and are thus excluded from the Nelder-Mead optimization, even though if they were optimized they would end up with higher likelihoods.
Another possibility, of course, is that the true parameter values are actually somewhat less likely than other places and, moreover, sit on a very steep, isolated peak in likelihood space, so that they are hard to find and hard to stay near.
I am going to explore this by initializing optimizations at the true parameter values and then seeing how the results of those optimizations compare with the results from the blind optimization.

Fig. \ref{fig:free-vs-truth-start} shows that, when you start the optimization algorithm at the true parameter values, the ML parameter estimates are closer to the truth (in almost every case) than those started blindly.

<<'free-vs-truth-start', fig.width=6, fig.height=4, fig.cap='Comparing the true parameter values (x-axis) against the parameter estimates when the optimization algorithm starts from naive guesses and when the algorithm starts at the true parameter values.'>>=
source("Growth_reproduction_trajectory_matching.R")
datasets <- readRDS("Trajectory_matching_datasets_8-9.RDS")

if (!file.exists("Trajectory_matching_estimates_starting_at_truth.RDS")) {
    results <- vector(mode='list', length=25)
    for (i in 1:25){
        print(i)
        optimizer(estpars=datasets[[i]]$params[c("Fh","rho","K","km","ER","Lobs","Robs")],
                  fixpars=datasets[[i]]$params[c("Imax","g","v","F0")],
                  parorder=c("Imax","Fh","g","rho","K","km","v","ER","F0","Lobs","Robs"),
                  transform=c("log","logit","logit",rep("log",4)),
                  obsdata=datasets[[i]]$data,
                  type="trajectory_matching",
              method="Nelder-Mead") -> results[[i]]
    }
    saveRDS(results, file="Trajectory_matching_estimates_starting_at_truth.RDS")
} else readRDS(file="Trajectory_matching_estimates_starting_at_truth.RDS") -> results

truth <- vector(mode='list', length=25)
for (i in 1:25){
    print(i)
    optimizer(estpars=datasets[[i]]$params[c("Fh","rho","K","km","ER","Lobs","Robs")],
              fixpars=datasets[[i]]$params[c("Imax","g","v","F0")],
              parorder=c("Imax","Fh","g","rho","K","km","v","ER","F0","Lobs","Robs"),
              transform=c("log","logit","logit",rep("log",4)),
              obsdata=datasets[[i]]$data,
              eval.only=TRUE,
              type="trajectory_matching",
              method="Nelder-Mead") -> truth[[i]]
}

estimates <- readRDS("Trajectory_matching_estimates_8-9.RDS")

data.frame(parameter=c("Fh","rho","K","km","ER","Lobs","Robs","lik"),
           set=rep(1:25,each=8),
           optimFree=lapply(estimates, function(e) e[1,1:8]) %>% unlist,
           optimTruth=lapply(results, function(r) c(r$params, r$lik)) %>% unlist,
           truth=lapply(truth, function(t) c(t$params, t$lik)) %>% unlist
           ) %>%
    gather(., key="likMethod", value="value", 3:4) -> out

ggplot(subset(out, parameter%in%c("Fh","rho","K","km","ER")), aes(x=truth, y=value, color=likMethod)) +
    geom_abline(slope=1, intercept=0) +
        geom_point() +
            facet_wrap(~parameter, scales="free")


@

More interesting, however, are the results of the likelihood calculation.
The likelihoods of the parameter estimates found either by the full optimization process or by starting at the true parameter values are so close that they lie on top of one another for almost every dataset (Fig. \ref{fig:lik-comparison}).
However, the likelihoods of the true parameter sets themselves are often much, much smaller.
This is most evident for parameter set 13, but you can see huge log-likelihood differences across the board.
However, it is also important to note that, in all but one case, the likelihood found by the full optimization is at least as good as the likelihood found by starting at the true parameter values (Fig. \ref{fig:lik-comparison-2}), although the differences are almost always very tiny.

<<'lik-comparison', fig.height=3, fig.width=4, fig.cap='Comparing the likelihood of the true parameter values to the likelihoods obtained by the fitting algorithm when initialized completely randomly and when initialized at the true parameter values.'>>=

data.frame(parameter=c("Fh","rho","K","km","ER","Lobs","Robs","lik"),
           set=rep(1:25,each=8),
           optimFree=lapply(estimates, function(e) e[1,1:8]) %>% unlist,
           optimTruth=lapply(results, function(r) c(r$params, r$lik)) %>% unlist,
           truth=lapply(truth, function(t) c(t$params, t$lik)) %>% unlist
           ) %>%
    gather(., key="likMethod", value="value", 3:5) %>%
        subset(., parameter=='lik')-> out2

ggplot(out2, aes(x=set, y=log(value), color=likMethod)) +
    geom_point() +
        xlab("Dataset") +
            ylab("Log-likelihood")


@

<<'lik-comparison-2', fig.height=3, fig.width=4, fig.cap='Comparing the log-likelihoods found via the optimization algorithm starting without any constraint versus that found from starting at the true parameter values. Red points indicate datasets where the likelihood was lower for the free start.'>>=
newd = data.frame(x=out2$value[out2$likMethod=='optimTruth'], y=out2$value[out2$likMethod=='optimFree'])
mutate(newd, col=factor(ifelse(y<x, 1, 0))) -> newd

ggplot(newd, aes(x=x, y=y, col=col)) +
        geom_abline(slope=1, intercept=0) +
            geom_point() +
                xlab("Log-likelihood, starting at the truth") +
                    ylab("Log-likelihood, starting randomly") +
                        scale_colour_manual(values=c('black','red')) +
                                theme_bw() +
                                    theme(legend.position='none')

@

\clearpage

Thus it appears to be generally true that there will be multiple parameter sets that give rise to nearly identical likelihoods, making it impossible to know, in general, which is closer to the truth.
However, the overall message from this fitting is pretty encouraging, as most of the parameter estimates are close to the truth and the algorithm is finding high likelihood parameter sets.
The problem of ridges in likelihood space is almost certain to be unavoidable.

Moreover, if I reduce the variability in the food addition (from a standard deviation of 15000 to just 5000, which is probably more realistic), the estimates improve further.
Fig. \ref{fig:rep-cost-est-3} shows the relative error in the parameter estimates for 25 parameter sets (note that all parameters are equal to the parameter sets above except for the variability in food addition).
As you can see, there is only one dataset with absolute relative error greater than one (dataset 13, which hugely overestimated the cost of reproduction and the observation error).
However, this is the parameter set that produced an individual that reproduced far more than any other, so this result is expected.
Otherwise, the parameter estimates look good for basically every parameter.

<<'rep-cost-est-3', fig.height=4, fig.width=6, fig.cap='Relative error in parameter estimates for 25 parameter sets when the variability in food addition is reduced.'>>=

datasets <- readRDS("Trajectory_matching_datasets_8-23.RDS")
results <- readRDS("Trajectory_matching_estimates_8-23.RDS")

library(dplyr)
library(tidyr)
library(ggplot2)

cbind(lapply(datasets, function(l) l$params[colnames(results[[1]])[1:7]]) %>%
          unlist %>%
              matrix(., ncol=7, byrow=TRUE, dimnames=list(as.character(1:25),colnames(results[[1]])[1:7])) %>%
                  as.data.frame %>%
                      mutate(., Fh=log(Fh)) %>%
                          gather(., key="parameter", value="truth", 1:7),
      lapply(results, function(l) head(l,1)[1:7]) %>%
          unlist %>%
              matrix(., ncol=7, byrow=TRUE, dimnames=list(as.character(1:25),colnames(results[[1]])[1:7])) %>%
                  as.data.frame %>%
                      mutate(., Fh=log(Fh)) %>%
                          gather(., key="parameter", value="estimate", 1:7)
      ) -> best
best <- best[,-3]

mutate(best, error=(estimate-truth)/truth) -> best
best$set <- rep(1:25, 7)

ggplot(best, aes(x=set, y=error)) +
    geom_point() +
        facet_wrap(~parameter, scales="free")

@

\end{document}